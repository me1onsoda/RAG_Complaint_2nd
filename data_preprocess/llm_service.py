import time
import re
import requests
import json
from app import database
from typing import List, Dict

OLLAMA_URL = "http://localhost:11434/api"
MODEL_NAME = "llama3.1"          
EMBED_MODEL = "mxbai-embed-large"

class LLMService:
    def preprocess(self, text):
        # íŠ¹ìˆ˜ë¬¸ìë§Œ ì œê±°í•˜ê³  ë¬¸ì¥ì€ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s.,!?]', '', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    async def get_normalization(self, body: str):
        """LLMì„ í˜¸ì¶œí•˜ì—¬ ë¯¼ì›ì„ ì •ê·œí™”ëœ JSON êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        
        # ì „ì²˜ë¦¬ ìˆ˜í–‰
        preprocess_body = self.preprocess(body)

        print(f"[*] ì „ì²˜ë¦¬ëœ ë¯¼ì› ë‚´ìš©: {preprocess_body}")

        prompt = f"""
            ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ êµ¬ì²­ì˜ ë¯¼ì› ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ë‹¤ìŒì€ ì‹œë¯¼ì´ ì ‘ìˆ˜í•œ ë¯¼ì› ë‚´ìš©ì…ë‹ˆë‹¤. ë‚´ìš©ì„ ì •í™•íˆ ì½ê³  ë°˜ë“œì‹œ 'í•œêµ­ì–´'ë¡œ ë¶„ì„í•˜ì„¸ìš”.

            [ë¯¼ì› ë‚´ìš©]
            {preprocess_body}

            [ì‘ë‹µ ì–‘ì‹]
            ë°˜ë“œì‹œ ì•„ë˜ì˜ JSON í˜•ì‹ì„ ì§€ì¼œì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            {{
            "neutral_summary": "ê°ê´€ì ì¸ ìš”ì•½ ë‚´ìš©",
            "core_request": "ë¯¼ì›ì¸ì˜ í•µì‹¬ ìš”êµ¬ì‚¬í•­",
            "core_cause": "ë¯¼ì› ë°œìƒ ì›ì¸",
            "target_object": ["ëŒ€ìƒë¬¼1", "ëŒ€ìƒë¬¼2"],
            "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]
            }}
            """
        
        # ì£¼ì˜: gemma2:2b ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤ (ollama pull gemma2:2b)
        response = requests.post(f"{OLLAMA_URL}/generate", json={
            "model": "gemma2:2b", 
            "prompt": prompt, 
            "format": "json", 
            "stream": False
        })

        if response.status_code == 200:
            # 3. LLMì˜ ì‘ë‹µ(JSON ë¬¸ìì—´)ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            result_dict = json.loads(response.json()['response'])
            
            # 4. ì „ì²˜ë¦¬ëœ ì›ë³¸ ë°ì´í„°ë¥¼ ê²°ê³¼ì— ì¶”ê°€
            result_dict['preprocess_body'] = preprocess_body
            
            return result_dict
        else:
            raise Exception(f"Ollama API Error: {response.text}")


    async def get_embedding(self, text: str):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. (mxbai-embed-large: 1024ì°¨ì›)"""
        # ì£¼ì˜: ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ì˜ ì°¨ì›ì´ 1024ì¸ì§€ í™•ì¸ í•„ìˆ˜!
        response = requests.post(f"{OLLAMA_URL}/embeddings", json={
            "model": EMBED_MODEL, 
            "prompt": text
        })
        if response.status_code == 200:
            return response.json()['embedding']
        else:
            raise Exception(f"Ollama Embedding Error: {response.text}")

    async def get_embedding_with_answer(self, complaint, department, answer):
        """ë¯¼ì›, ë¶€ì„œëª…, ë‹µë³€ì„ ê²°í•©í•˜ì—¬ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        combined_text = f"ë¯¼ì›: {complaint}\në¶€ì„œëª…: {department}\në‹µë³€: {answer}"
        return await self.get_embedding(combined_text)


    async def optimize_query(self, user_query: str) -> str:
        """ì‚¬ìš©ìì˜ ë¹„ì •í˜• ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ ë²•ë¥ /í–‰ì • ìš©ì–´ë¡œ ë³€í™˜

        ì˜ˆ: 'ìŠ¤ì¿¨ì¡´ ë”±ì§€ ì–¼ë§ˆ' -> 'ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­ ë¶ˆë²•ì£¼ì •ì°¨ ê³¼íƒœë£Œ ë¶€ê³¼ ê¸°ì¤€'

        Args:
            user_query (str): ê³µë¬´ì›ì˜ ì›ë³¸ ì§ˆë¬¸

        Returns:
            str: ìµœì í™”ëœ ê²€ìƒ‰ì–´ í…ìŠ¤íŠ¸
        """
        prompt = f"""
        ê³µë¬´ì›ì´ ì…ë ¥í•œ 'ì‹¤ë¬´ ê²€ìƒ‰ì–´'ë¥¼ 'ë²•ë¥ /í–‰ì • í‘œì¤€ ìš©ì–´'ë¡œ ë³€í™˜í•˜ì„¸ìš”.
        - ì•½ì–´ëŠ” ì •ì‹ ëª…ì¹­ìœ¼ë¡œ (ìŠ¤ì¿¨ì¡´ -> ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­)
        - ë¬¸ë§¥ì„ ë³´ê°•í•˜ì—¬ ëª…í™•í•˜ê²Œ (ë¹¨ê°„ í†µ -> ì†Œë°©ìš©ìˆ˜ì‹œì„¤)
        - ì„¤ëª… ì—†ì´ ë³€í™˜ëœ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

        ì…ë ¥: "{user_query}"
        ì¶œë ¥:
        """
        try:
            response = requests.post(f"{OLLAMA_URL}/generate", json={
                "model": MODEL_NAME, "prompt": prompt, "stream": False
            })
            return response.json()['response'].strip()
        except Exception as e:
            print(f"âš ï¸ ì¿¼ë¦¬ ìµœì í™” ì‹¤íŒ¨ (ì›ë³¸ ì‚¬ìš©): {e}")
            return user_query

    def _clip(self, text: str, max_chars: int) -> str:
        """í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì œí•œí•˜ì—¬ í”„ë¡¬í”„íŠ¸ í† í° ì´ˆê³¼ë¥¼ ë°©ì§€

        Args:
            text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            max_chars (int): ìµœëŒ€ í—ˆìš© ê¸€ì ìˆ˜

        Returns:
            str: ì˜ë¦° í…ìŠ¤íŠ¸ (... í¬í•¨)
        """
        if not text: return ""
        text = text.strip()
        if len(text) <= max_chars:
            return text
        return text[:max_chars].rstrip() + "..."

    async def retrieve_references(self, complaint_id: int, query: str = None) -> Dict[str, List]:
        """ìƒí™©(ìë™/ìˆ˜ë™)ì— ë§ì¶° DBì—ì„œ ì ì ˆí•œ ì°¸ê³  ìë£Œë¥¼ ê²€ìƒ‰

        - queryê°€ ì—†ìŒ: ë¯¼ì› ID ê¸°ë°˜ ìë™ ì¶”ì²œ (Context Search)
        - queryê°€ ìˆìŒ: ì±„íŒ… ì§ˆë¬¸ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Vector + Keyword)

        Args:
            complaint_id (int): í˜„ì¬ ë³´ê³  ìˆëŠ” ë¯¼ì› ID
            query (str, optional): ì‚¬ìš©ìì˜ ì¶”ê°€ ì§ˆë¬¸

        Returns:
            Dict: 'cases'(ìœ ì‚¬ì‚¬ë¡€), 'laws'(ê´€ë ¨ë²•ë ¹) ë¦¬ìŠ¤íŠ¸ í¬í•¨
        """
        references = { "cases": [], "laws": [] }

        if query is None:
            print(f"ğŸ¤– [Auto] ë¯¼ì› #{complaint_id} ìë™ ì¶”ì²œ ê²€ìƒ‰ ìˆ˜í–‰")
            references["cases"] = database.search_cases_by_id(complaint_id)
            references["laws"] = database.search_laws_by_id(complaint_id)
        else:
            print(f"ğŸ‘¤ [Manual] ì‚¬ìš©ì ì§ˆì˜ ê²€ìƒ‰: {query}")
            
            # 1. ì¿¼ë¦¬ ìµœì í™” (ì‹¤ë¬´ ìš©ì–´ -> ë²•ë¥  ìš©ì–´)
            # [ìˆ˜ì •] self.optimize_query í˜¸ì¶œ
            refined_query = await self.optimize_query(query)
            print(f"   -> ë³€í™˜ëœ ì¿¼ë¦¬: {refined_query}")
            
            # 2. ì„ë² ë”© ìƒì„±
            # [ìˆ˜ì •] self.get_embedding í˜¸ì¶œ
            query_vec = await self.get_embedding(refined_query)
            
            # 3. ê²€ìƒ‰ ìˆ˜í–‰ (ë²•ë ¹ì€ í‚¤ì›Œë“œ í•„í„°ë§ í¬í•¨)
            references["cases"] = database.search_cases_by_text(query_vec)
            references["laws"] = database.search_laws_by_text(query_vec, keyword=refined_query)

        return references

    async def generate_rag_response(self, complaint_id: int, user_query: str = None) -> str:
        """RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±

        Retrieval(ê²€ìƒ‰) -> Augmentation(í”„ë¡¬í”„íŠ¸ ì¡°ë¦½) -> Generation(ìƒì„±) ë‹¨ê³„ë¥¼ ê±°ì¹©ë‹ˆë‹¤.

        Args:
            complaint_id (int): ëŒ€ìƒ ë¯¼ì› ID
            user_query (str, optional): ì‚¬ìš©ì ì§ˆë¬¸ (ì—†ìœ¼ë©´ ìë™ ìš”ì•½ ëª¨ë“œ)

        Returns:
            str: LLMì´ ìƒì„±í•œ ìµœì¢… ë‹µë³€ í…ìŠ¤íŠ¸
        """
        # 1. Retrieval: ê·¼ê±° ìë£Œ ê²€ìƒ‰
        # [ìˆ˜ì •] self.retrieve_references í˜¸ì¶œ
        references = await self.retrieve_references(complaint_id, user_query)
        cases = references['cases']
        laws = references['laws']

        if not cases and not laws:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ìœ ì‚¬ ì‚¬ë¡€ë‚˜ ë²•ë ¹ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 2. Augmentation: ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½ (ê¸¸ì´ ì œí•œ ì ìš©)
        context_text = "## 1. ìœ ì‚¬ ë¯¼ì› ì²˜ë¦¬ ì‚¬ë¡€\n"
        for i, case in enumerate(cases[:3], 1): # Top-3 ì œí•œ
            # [ìˆ˜ì •] self._clip í˜¸ì¶œ
            context_text += (
                f"[{i}] {case['summary']} (ìœ ì‚¬ë„: {case['similarity']}%)\n"
                f"   - ì²˜ë¦¬ê²°ê³¼: {self._clip(case['answer'], 200)}\n"
            )
        
        context_text += "\n## 2. ê´€ë ¨ ë²•ë ¹\n"
        for i, law in enumerate(laws[:3], 1): # Top-3 ì œí•œ
            context_text += (
                f"[{i}] {law['title']} {law['section']} (ìœ ì‚¬ë„: {law['similarity']}%)\n"
                f"   - ë‚´ìš©: {self._clip(law['content'], 600)}\n"
            )

        # ìƒí™©ë³„ í”„ë¡¬í”„íŠ¸ ë¶„ê¸°
        if user_query:
            system_instruction = f"""
            ë‹¹ì‹ ì€ ê³µë¬´ì› ì—…ë¬´ ì§€ì› AIì…ë‹ˆë‹¤. ì•„ë˜ [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            
            [ì‘ì„± ê·œì¹™]
            1. ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  'ìë£Œì—ì„œ í™•ì¸ ë¶ˆê°€'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.
            2. ë‹µë³€ ëì— ë°˜ë“œì‹œ ì¸ìš© ì¶œì²˜ë¥¼ ëŒ€ê´„í˜¸ë¡œ í‘œê¸°í•˜ì„¸ìš” (ì˜ˆ: [ë„ë¡œêµí†µë²• ì œ32ì¡° ì°¸ê³ ]).
            3. ê³µì†í•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

            [ì§ˆë¬¸]: "{user_query}"
            """
        else:
            system_instruction = f"""
            ë‹¹ì‹ ì€ ë¯¼ì› ë¶„ì„ AIì…ë‹ˆë‹¤. [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¯¼ì› #{complaint_id}ì˜ ì²˜ë¦¬ ë°©í–¥ ê°€ì´ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            
            [ì‘ì„± ê·œì¹™]
            1. ìœ ì‚¬ ì‚¬ë¡€ë“¤ì´ ì£¼ë¡œ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ ìš”ì•½í•˜ì„¸ìš”.
            2. ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ ë²•ë ¹ì´ ë¬´ì—‡ì¸ì§€ ì§šì–´ì£¼ì„¸ìš”.
            3. ë‹´ë‹¹ìì—ê²Œ ì¶”ì²œí•˜ëŠ” ì²˜ë¦¬ ë°©í–¥ì„ 3ì¤„ ì´ë‚´ë¡œ ì œì•ˆí•˜ì„¸ìš”.
            """

        final_prompt = f"{system_instruction}\n\n[ì°¸ê³  ìë£Œ]\n{context_text}"

        # 3. Generation: ë‹µë³€ ìƒì„±
        try:
            response = requests.post(f"{OLLAMA_URL}/generate", json={
                "model": MODEL_NAME, "prompt": final_prompt, "stream": False
            })
            return response.json()['response'].strip()
        except Exception as e:
            print(f"âŒ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"