import psycopg2
import pandas as pd
import numpy as np
import json
import ast
import re
from datetime import datetime
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter

# DB ì„¤ì •
DB_CONFIG = { "host": "localhost", "dbname": "complaint_db", "user": "postgres", "password": "0000", "port": "5432" }

def parse_vector(val):
    if isinstance(val, str):
        try: return np.array(json.loads(val))
        except: return np.zeros(1024)
    return np.array(val) if val is not None else np.zeros(1024)

def parse_keywords(val):
    if not val: return set()
    raw_set = set()
    if isinstance(val, str):
        try: raw_set = set(json.loads(val))
        except: 
            try: raw_set = set(ast.literal_eval(val))
            except: raw_set = set()
    else: raw_set = set(val)
    
    stop_words = {
        'í•­ìƒ', 'ì§„ì§œ', 'ë„ˆë¬´', 'ë§¤ì¼', 'ìê¾¸', 'ê´€ë¦¬', 'ë¯¼ì›', 'êµ¬ì²­', 'ì‹œì¥', 'ì‚¬í•­', 'ë¶ˆí¸', 'ìš”ì²­',
        'ë¬¸ì˜', 'ì‹ ê³ ', 'ëŒ€í•˜ì—¬', 'ê´€ë ¨', 'ë‹µë³€', 'ë¶€íƒ', 'ì ‘ìˆ˜', 'ì¡°ì¹˜', 'í™•ì¸', 'ë‚´ìš©', 'ì§„í–‰', 'ë°”ëë‹ˆë‹¤',
        'ì£¼ë¯¼ì„¼í„°', 'ì§ì›', 'ì¹œì ˆ', 'ë¶ˆì¹œì ˆ', 'ê°ì‚¬' # ë„ˆë¬´ í”í•œ ë§ ì œì™¸
    }

    cleaned_set = set()
    for word in raw_set:
        # í•œê¸€ë§Œ ì¶”ì¶œ
        korean_word = re.sub('[^ê°€-í£]', '', word)
        if len(korean_word) >= 2:
            cleaned_set.add(korean_word)
            
    return cleaned_set

def get_representative_keyword(keywords_list):
    all_kws = [kw for sub in keywords_list for kw in sub]
    if not all_kws: return "ë¯¼ì›"
    top_kw = Counter(all_kws).most_common(1)[0][0]
    return str(top_kw).strip()

# [í•µì‹¬] ìŠ¤ë§ˆíŠ¸ ì œëª© ìƒì„± (ì¤‘ë³µ íšŒí”¼ ë¡œì§ ì¶”ê°€)
def generate_unique_smart_title(group, centroid_vec, existing_titles):
    """
    ì œëª©ì„ ë§Œë“¤ê³  existing_titles(ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì œëª©ë“¤)ì™€ ê²¹ì¹˜ë©´
    ë’¤ì— êµ¬ì²´ì ì¸ ì •ë³´(í‚¤ì›Œë“œ, ë‚ ì§œ)ë¥¼ ë¶™ì—¬ì„œ ìœ ë‹ˆí¬í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    """
    
    # 1. ê¸°ë³¸ í›„ë³´: ë°˜ì¥ ë¯¼ì›ì˜ í•µì‹¬ ìš”ì•½
    candidate_title = "ë³µí•© ë¯¼ì›"
    
    # 1. í•µì‹¬ ìš”ì²­ì‚¬í•­(core_request) ìš”ì•½ ì‹œë„
    if not group.empty:
        # ê°€ì¥ ê¸´ core_requestë¥¼ ê°€ì§„ í–‰ì„ ì°¾ìŒ (ì •ë³´ëŸ‰ì´ ë§ì„ í™•ë¥  ë†’ìŒ)
        best_row = group.loc[group['core_request'].str.len().sort_values(ascending=False).index[0]]
        summary = best_row.get('core_request', '')

        if summary and 5 < len(summary) < 40:
             candidate_title = summary.replace('\n', ' ').strip()
        else:
            # 2. ì‹¤íŒ¨ì‹œ í‚¤ì›Œë“œ ì¡°í•©
            all_kws = []
            for kws in group['kws']: all_kws.extend(list(kws))
            counts = Counter(all_kws)
            if counts:
                top_kws = [w for w, c in counts.most_common(3)]
                if len(top_kws) >= 2:
                    candidate_title = f"{top_kws[0]}, {top_kws[1]} ê´€ë ¨ ë¯¼ì›"
                elif top_kws:
                    candidate_title = f"{top_kws[0]} ê´€ë ¨ ìš”ì²­"

    # [ì¤‘ë³µ ê²€ì‚¬ ë° íšŒí”¼ ê¸°ë™]
    # ë§Œì•½ ì´ ì œëª©ì´ ì´ë¯¸ ì¡´ì¬í•œë‹¤ë©´?
    base_title = candidate_title
    retry_count = 0
    
    while candidate_title in existing_titles:
        retry_count += 1
        candidate_title = f"{base_title} ({retry_count})"
    return candidate_title

def run_cumulative_clustering():
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    print(f"ğŸš€ [System] 'ë°¸ëŸ°ìŠ¤ íŒ¨ì¹˜' êµ°ì§‘í™” ì‹œì‘ (ìœ ì‚¬ë„ 0.85 / í‚¤ì›Œë“œ ì™„í™”) ({datetime.now()})")

    # 1. ê¸°ì¡´ êµ°ì§‘ ì •ë³´ ë¡œë“œ
    sql_active = """
        SELECT c.incident_id, n.embedding, n.keywords_jsonb, i.title,
               (SELECT COUNT(*) FROM complaints WHERE incident_id = c.incident_id) as member_count
        FROM complaints c
        JOIN complaint_normalizations n ON c.id = n.complaint_id
        JOIN incidents i ON c.incident_id = i.id
        WHERE c.incident_id IS NOT NULL
    """
    active_df = pd.read_sql(sql_active, conn)
    active_df['vec'] = active_df['embedding'].apply(parse_vector)
    active_df['kws'] = active_df['keywords_jsonb'].apply(parse_keywords)
    
    incident_centroids = []
    incident_ids = []
    incident_kws = []
    existing_titles = set()
    
    if not active_df.empty:
        existing_titles.update(active_df['title'].unique())
        for iid, group in active_df.groupby('incident_id'):
            # 50ê°œ ì´ìƒì¸ ë°©ì€ ë” ì´ìƒ ë°›ì§€ ì•ŠìŒ (ì“°ë ˆê¸°í†µ ë°©ì§€)
            if group.iloc[0]['member_count'] >= 50:
                continue

            vectors = np.stack(group['embedding'].apply(parse_vector).values)
            incident_centroids.append(np.mean(vectors, axis=0))
            incident_ids.append(iid)

            # [ì™„í™”] êµì§‘í•© ëŒ€ì‹ , ê°€ì¥ ë¹ˆë„ ë†’ì€ í‚¤ì›Œë“œ ìƒìœ„ 5ê°œë¥¼ ëŒ€í‘œ í‚¤ì›Œë“œë¡œ ì„ ì •
            all_kws = []
            for k in group['keywords_jsonb'].apply(parse_keywords):
                all_kws.extend(list(k))

            common_kws = set([w for w, c in Counter(all_kws).most_common(10)])
            incident_kws.append(common_kws)

    # 2. ë¯¸ë°°ì • ë¯¼ì› ë¡œë“œ
    sql_unassigned = """
        SELECT c.id, c.received_at, n.embedding, n.keywords_jsonb, n.core_request
        FROM complaints c
        JOIN complaint_normalizations n ON c.id = n.complaint_id
        WHERE c.incident_id IS NULL AND n.embedding IS NOT NULL
    """
    target_df = pd.read_sql(sql_unassigned, conn)
    
    if target_df.empty:
        print("ğŸ‰ ëŒ€ê¸° ì¤‘ì¸ ë¯¼ì›ì´ ì—†ìŠµë‹ˆë‹¤."); conn.close(); return

    print(f"ğŸ‘‰ ëŒ€ê¸° ë¯¼ì› {len(target_df)}ê±´ ì²˜ë¦¬ ì¤‘...")
    target_df['vec'] = target_df['embedding'].apply(parse_vector)
    target_df['kws'] = target_df['keywords_jsonb'].apply(parse_keywords)
    print(f"ğŸ‘‰ ëŒ€ê¸°/ì‹ ê·œ ë¯¼ì› {len(target_df)}ê±´ ë¶„ë¥˜ ì‹œì‘...")

    # 3. ë§¤ì¹­ í”„ë¡œì„¸ìŠ¤
    assigned_count = 0
    unassigned_indices = []

    # [ì„¤ì • ì™„í™”] 0.85 = "ë¬¸ì¥ì€ ë‹¤ë¥´ì§€ë§Œ ì£¼ì œëŠ” ê°™ìŒ" ìˆ˜ì¤€
    BALANCE_THRESHOLD = 0.85

    # 3. ê¸°ì¡´ ë°© ì…ì¥ ë¡œì§
    if incident_centroids:
        target_vecs = np.stack(target_df['vec'].values)
        anchor_vecs = np.stack(incident_centroids)
        sim_matrix = cosine_similarity(target_vecs, anchor_vecs)
        
        for idx in range(len(target_df)):
            row = target_df.iloc[idx]
            best_idx = -1
            max_sim = -1.0

            for a_idx in range(len(incident_ids)):
                # [ì™„í™”] í‚¤ì›Œë“œê°€ 1ê°œë¼ë„ ê²¹ì¹˜ë©´ OK
                target_k = row['kws']
                anchor_k = incident_kws[a_idx]

                if not (target_k & anchor_k): # êµì§‘í•© ì—†ìœ¼ë©´ íŒ¨ìŠ¤
                    if len(target_k) > 0 and len(anchor_k) > 0:
                        continue
                    # í‚¤ì›Œë“œ ì¶”ì¶œì´ ì•ˆ ëœ ê²½ìš°ëŠ” ë²¡í„°ë§Œ ë¯¿ê³  ì§„í–‰

                sim = sim_matrix[idx][a_idx]
                if sim > max_sim:
                    max_sim = sim
                    best_idx = a_idx

            if max_sim >= BALANCE_THRESHOLD:
                best_iid = incident_ids[best_idx]
                cur.execute("UPDATE complaints SET incident_id = %s WHERE id = %s", (int(best_iid), int(row['id'])))
                cur.execute("UPDATE incidents SET closed_at = GREATEST(closed_at, %s) WHERE id = %s", (row['received_at'], int(best_iid)))
                assigned_count += 1
            else:
                unassigned_indices.append(idx)
    else:
        unassigned_indices = list(range(len(target_df)))

    conn.commit()

    # 4. ì‹ ê·œ ê·¸ë£¹ í˜•ì„± (DBSCAN)
    new_group_count = 0
    single_room_count = 0

    if unassigned_indices:
        remaining_df = target_df.iloc[unassigned_indices].copy()

        # [ì„¤ì • ì™„í™”] eps=0.12 (ì•½ê°„ì˜ í‘œí˜„ ì°¨ì´ í—ˆìš©), min_samples=2
        if len(remaining_df) >= 2:
            final_vecs = np.stack(remaining_df['vec'].values)
            # epsë¥¼ 0.12ë¡œ ëŠ˜ë ¤ì„œ "ë¹„ìŠ·í•˜ë©´" ë¬¶ì´ê²Œ í•¨
            dbscan = DBSCAN(eps=0.12, min_samples=2, metric='cosine')
            labels = dbscan.fit_predict(final_vecs)
            remaining_df['label'] = labels
        else:
            remaining_df['label'] = -1

        for label in set(remaining_df['label']):
            if label != -1:
                cls = remaining_df[remaining_df['label'] == label]

                # í‚¤ì›Œë“œ ê²€ì‚¬ë„ ì™„í™” (êµì§‘í•© ì—†ì–´ë„ ë²¡í„°ê°€ ë§¤ìš° ê°€ê¹Œìš°ë©´ í—ˆìš©)
                # ë‹¤ë§Œ, ì™„ì „íˆ ì—‰ëš±í•œê²Œ ë¬¶ì´ëŠ”ê±¸ ë°©ì§€í•˜ê¸° ìœ„í•´
                # ë²¡í„°ë“¤ì˜ í‰ê·  ê±°ë¦¬ê°€ ë„ˆë¬´ ë©€ë©´ ì°¢ëŠ” ë¡œì§ì€ ìƒëµ (epsê°€ ì œì–´í•¨)

                centroid = np.mean(np.stack(cls['vec'].values), axis=0)
                title = generate_unique_smart_title(cls, centroid, existing_titles)
                existing_titles.add(title)

                cur.execute("INSERT INTO incidents (title, status, opened_at, closed_at) VALUES (%s, 'OPEN', %s, %s) RETURNING id",
                           (title, cls['received_at'].min(), cls['received_at'].max()))
                new_iid = cur.fetchone()[0]
                cur.execute(f"UPDATE complaints SET incident_id = %s WHERE id IN %s", (new_iid, tuple(cls['id'].tolist())))
                new_group_count += 1

            else:
                # 1ì¸ì‹¤ (í™”ë©´ì—ëŠ” ì•ˆ ë„ìš¸ ì˜ˆì •)
                noises = remaining_df[remaining_df['label'] == -1]
                for _, row in noises.iterrows():
                    temp_df = pd.DataFrame([row])
                    title = generate_unique_smart_title(temp_df, None, existing_titles)
                    existing_titles.add(title)

                    cur.execute("INSERT INTO incidents (title, status, opened_at, closed_at) VALUES (%s, 'OPEN', %s, %s) RETURNING id",
                               (title, row['received_at'], row['received_at']))
                    new_iid = cur.fetchone()[0]
                    cur.execute("UPDATE complaints SET incident_id = %s WHERE id = %s", (new_iid, int(row['id'])))
                    single_room_count += 1

    conn.commit()
    cur.close(); conn.close()

    print(f"âœ… ê²°ê³¼ ìš”ì•½:")
    print(f"  - ê¸°ì¡´ ë°© í¡ìˆ˜: {assigned_count}ê±´")
    print(f"  - ì‹ ê·œ ê·¸ë£¹ ìƒì„±: {new_group_count}ê°œ (ì—¬ê¸°ì— ì£¼ëª©í•˜ì„¸ìš”!)")
    print(f"  - 1ì¸ ëŒ€ê¸°ë°©: {single_room_count}ê°œ (í™”ë©´ í•„í„°ë§ í•„ìš”)")

if __name__ == "__main__":
    run_cumulative_clustering()