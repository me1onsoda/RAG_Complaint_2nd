import os
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings # ë§Œì•½ OpenAI ì„ë² ë”©ì„ ì¼ë‹¤ë©´ ì´ê±°
from langchain_ollama import ChatOllama
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (API í‚¤ ë“±)
load_dotenv()

# ==========================================
# [ì¤‘ìš”] ì—¬ê¸°ëŠ” test_search.pyì™€ ì„¤ì •ì´ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤!
# ==========================================
# ì €ì¥ëœ DB ê²½ë¡œ (test_search.pyì— ìˆëŠ” ê²½ë¡œ í™•ì¸)
PERSIST_DIRECTORY = "./db"  
# ì„ë² ë”© ëª¨ë¸ (ë°ì´í„° ì €ì¥í•  ë•Œ ì“´ ê²ƒê³¼ ê°™ì€ ê²ƒì´ì–´ì•¼ í•¨)
embedding_model = OpenAIEmbeddings() 
# ==========================================

def run_rag():
    print("ğŸ“‚ 1. ë²¡í„° DBë¥¼ ì—°ê²°í•˜ëŠ” ì¤‘...")
    if not os.path.exists(PERSIST_DIRECTORY):
        print("âŒ DB í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ingest.pyë¥¼ ë¨¼ì € ì‹¤í–‰í–ˆë‚˜ìš”?")
        return

    # DB ë¶ˆëŸ¬ì˜¤ê¸°
    vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding_model)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # ê´€ë ¨ ë¬¸ì„œ 3ê°œ ì°¾ê¸°

    print("ğŸ¤– 2. Llama(AI)ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘...")
    llm = ChatOllama(model="llama3.1", temperature=0)

    # ì§ˆë¬¸í•˜ê¸°
    question = "ì—¬ê¶Œ ë°œê¸‰ì€ ì–´ë””ì„œ í•´?"
    print(f"\nâ“ ì§ˆë¬¸: {question}")
    
    # 3. ê²€ìƒ‰í•˜ê¸° (Retrieve)
    print("ğŸ” ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ëŠ” ì¤‘...")
    docs = retriever.invoke(question)
    
    if not docs:
        print("âŒ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # ì°¾ì€ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬
    context = "\n".join([doc.page_content for doc in docs])
    print(f"ğŸ“„ ì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©(ì¼ë¶€):\n{context[:200]}...") # ë‚´ìš© ì‚´ì§ ë³´ì—¬ì£¼ê¸°

    # 4. AIì—ê²Œ ì‹œí‚¤ê¸° (Prompt)
    # "ë„ˆëŠ” ìƒë‹´ì›ì´ì•¼. ì•„ë˜ [ì •ë³´]ë¥¼ ë³´ê³  ì§ˆë¬¸ì— ë‹µí•´." ë¼ê³  ì‹œí‚´
    template = """
    ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë¯¼ì› ì•ˆë‚´ ê³µë¬´ì›ì…ë‹ˆë‹¤.
    ì•„ë˜ì˜ [ì°¸ê³  ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³  "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”.

    [ì°¸ê³  ì •ë³´]
    {context}

    ì§ˆë¬¸: {question}
    """
    
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm

    # 5. ìµœì¢… ë‹µë³€ ìƒì„±
    print("\nğŸ’¬ AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    response = chain.invoke({"context": context, "question": question})

    print("-" * 30)
    print("âœ… ìµœì¢… ë‹µë³€:")
    print(response.content)
    print("-" * 30)

if __name__ == "__main__":
    run_rag()