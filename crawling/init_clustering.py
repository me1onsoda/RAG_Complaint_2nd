import psycopg2
import pandas as pd
import numpy as np
import json
import ast
import re
from datetime import datetime
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_distances
from collections import Counter

# DB ì„¤ì •
DB_CONFIG = { "host": "localhost", "dbname": "complaint_db", "user": "postgres", "password": "0000", "port": "5432" }

def parse_vector(val):
    if isinstance(val, str):
        try: return np.array(json.loads(val))
        except: return np.zeros(1024)
    return np.array(val) if val is not None else np.zeros(1024)

def parse_keywords(val):
    if not val: return set()
    raw_set = set()
    if isinstance(val, str):
        try: raw_set = set(json.loads(val))
        except: 
            try: raw_set = set(ast.literal_eval(val))
            except: raw_set = set()
    else: raw_set = set(val)
    
    # [ìˆ˜ì •] í•œê¸€ 2ê¸€ì ì´ìƒë§Œ ë‚¨ê¸°ê³ , ì˜ì–´/íŠ¹ìˆ˜ë¬¸ì ì œê±° (Environment ê°™ì€ê±° ì œê±°)
    cleaned_set = set()
    for word in raw_set:
        # í•œê¸€ë§Œ ì¶”ì¶œ
        korean_word = re.sub('[^ê°€-í£]', '', word)
        if len(korean_word) >= 2:
            cleaned_set.add(korean_word)
            
    return cleaned_set

def get_representative_keyword(keywords_list):
    all_kws = [kw for sub in keywords_list for kw in sub]
    if not all_kws: return "ë¯¼ì›"
    top_kw = Counter(all_kws).most_common(1)[0][0]
    return str(top_kw).strip()

# [í•µì‹¬] ìŠ¤ë§ˆíŠ¸ ì œëª© ìƒì„± (ì¤‘ë³µ íšŒí”¼ ë¡œì§ ì¶”ê°€)
def generate_unique_smart_title(group, centroid_vec, existing_titles):
    """
    ì œëª©ì„ ë§Œë“¤ê³  existing_titles(ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì œëª©ë“¤)ì™€ ê²¹ì¹˜ë©´
    ë’¤ì— êµ¬ì²´ì ì¸ ì •ë³´(í‚¤ì›Œë“œ, ë‚ ì§œ)ë¥¼ ë¶™ì—¬ì„œ ìœ ë‹ˆí¬í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    """
    
    # 1. ê¸°ë³¸ í›„ë³´: ë°˜ì¥ ë¯¼ì›ì˜ í•µì‹¬ ìš”ì•½
    candidate_title = "ë³µí•© ë¯¼ì›"
    
    # ë°˜ì¥ ì„ ì¶œ
    if centroid_vec is not None:
        vectors = np.stack(group['vec'].values)
        dists = cosine_distances([centroid_vec], vectors)[0]
        best_idx = np.argmin(dists)
        leader_row = group.iloc[best_idx]
        
        summary = leader_row.get('core_request', '')
        if summary and 3 < len(summary) < 50:
             candidate_title = summary.replace('\n', ' ').strip()
        else:
            # ë°˜ì¥ ìš”ì•½ì´ ë³„ë¡œë©´ í‚¤ì›Œë“œ ì¡°í•© ì‹œë„
            all_kws = []
            for kws in group['kws']: all_kws.extend(list(kws))
            counts = Counter(all_kws)
            # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ê°•í™”
            stop_words = {'ë¯¼ì›', 'ìš”ì²­', 'ë¬¸ì˜', 'ì‹ ê³ ', 'ëŒ€í•˜ì—¬', 'ê´€ë ¨', 'ë‹µë³€', 'ë¶€íƒ', 'ë¶ˆí¸', 'ì ‘ìˆ˜', 'ì‚¬í•­', 'êµ¬ì²­', 'ì‹œì¥'}
            top_kws = [word for word, count in counts.most_common(10) if word not in stop_words]
            
            if len(top_kws) >= 2: candidate_title = f"{top_kws[0]}, {top_kws[1]} ê´€ë ¨ ë¯¼ì›"
            elif len(top_kws) == 1: candidate_title = f"{top_kws[0]} ê´€ë ¨ ë¯¼ì›"

    # [ì¤‘ë³µ ê²€ì‚¬ ë° íšŒí”¼ ê¸°ë™]
    # ë§Œì•½ ì´ ì œëª©ì´ ì´ë¯¸ ì¡´ì¬í•œë‹¤ë©´?
    base_title = candidate_title
    retry_count = 0
    
    while candidate_title in existing_titles:
        retry_count += 1
        
        # ì „ëµ A: ê°€ì¥ ë¹ˆë„ ë†’ì€ 'ì¥ì†Œ'ë‚˜ 'ëª…ì‚¬' í‚¤ì›Œë“œë¥¼ ë’¤ì— ë¶™ì„
        all_kws = []
        for kws in group['kws']: all_kws.extend(list(kws))
        counts = Counter(all_kws)
        # ì´ë¯¸ ì œëª©ì— í¬í•¨ëœ ë‹¨ì–´ëŠ” ì œì™¸í•˜ê³  ì¶”ì²œ
        extras = [w for w, c in counts.most_common(10) if w not in base_title]
        
        if len(extras) >= retry_count:
            # ì˜ˆ: "ì“°ë ˆê¸° ìˆ˜ê±° ìš”ì²­" -> "ì“°ë ˆê¸° ìˆ˜ê±° ìš”ì²­ (ê³ ë•ë™)"
            candidate_title = f"{base_title} ({extras[retry_count-1]})"
        else:
            # ì „ëµ B: í‚¤ì›Œë“œë„ ë‹¤ ì¼ìœ¼ë©´ ë‚ ì§œë¥¼ ë¶™ì„
            # ì˜ˆ: "ì“°ë ˆê¸° ìˆ˜ê±° ìš”ì²­ (01/15)"
            date_str = group['received_at'].min().strftime("%m/%d")
            candidate_title = f"{base_title} ({date_str})"
            
            # ì „ëµ C: ë‚ ì§œë„ ê²¹ì¹˜ë©´ ì•„ì˜ˆ IDë¥¼ ë¶™ì—¬ë²„ë¦¼ (ìµœí›„ì˜ ìˆ˜ë‹¨)
            if candidate_title in existing_titles:
                 candidate_title = f"{base_title} #{retry_count}"

    return candidate_title

def calculate_hybrid_distance(vec1, vec2, kws1, kws2):
    sem_dist = cosine_distances([vec1], [vec2])[0][0]
    if not kws1 and not kws2: key_dist = 0.5
    elif not kws1 or not kws2: key_dist = 1.0
    else:
        inter = len(kws1.intersection(kws2))
        union = len(kws1.union(kws2))
        key_dist = 1.0 - (inter / union if union else 0)
    return (sem_dist * 0.8) + (key_dist * 0.2)

def run_cumulative_clustering():
    conn = psycopg2.connect(**DB_CONFIG)
    cur = conn.cursor()
    print(f"ğŸš€ [System] ì¤‘ë³µ ë°©ì§€ ìŠ¤ë§ˆíŠ¸ êµ°ì§‘í™” ì‹œì‘ ({datetime.now()})")

    # 1. ì•µì»¤ ë¡œë“œ
    sql_active = """
        SELECT c.incident_id, n.embedding, n.keywords_jsonb, i.title
        FROM complaints c
        JOIN complaint_normalizations n ON c.id = n.complaint_id
        JOIN incidents i ON c.incident_id = i.id
        WHERE c.incident_id IS NOT NULL 
    """
    # incidents í…Œì´ë¸”ê³¼ ì¡°ì¸í•´ì„œ ì´ë¯¸ ìˆëŠ” ì œëª©ë“¤ì„ ê°€ì ¸ì˜´
    
    active_df = pd.read_sql(sql_active, conn)
    active_df['vec'] = active_df['embedding'].apply(parse_vector)
    active_df['kws'] = active_df['keywords_jsonb'].apply(parse_keywords)
    
    incident_centroids = {}
    
    # [ì¤‘ìš”] ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì œëª©ë“¤ì„ ê¸°ì–µí•˜ëŠ” ì§‘í•©(Set)
    existing_titles = set()
    
    if not active_df.empty:
        # ê¸°ì¡´ ì œëª©ë“¤ ë“±ë¡
        existing_titles.update(active_df['title'].dropna().unique())
        
        for iid, group in active_df.groupby('incident_id'):
            mean_vec = np.mean(np.stack(group['vec'].values), axis=0)
            all_kws = set().union(*group['kws'].tolist())
            incident_centroids[iid] = {'vec': mean_vec, 'kws': all_kws, 'count': len(group)}

    # 2. ë¯¸ë°°ì • ë¯¼ì› ë¡œë“œ
    sql_unassigned = """
        SELECT c.id, c.received_at, n.embedding, n.keywords_jsonb, n.core_request
        FROM complaints c
        JOIN complaint_normalizations n ON c.id = n.complaint_id
        WHERE c.incident_id IS NULL AND n.embedding IS NOT NULL
    """
    target_df = pd.read_sql(sql_unassigned, conn)
    
    if target_df.empty:
        print("ğŸ‰ ëŒ€ê¸° ì¤‘ì¸ ë¯¼ì›ì´ ì—†ìŠµë‹ˆë‹¤."); conn.close(); return

    target_df['vec'] = target_df['embedding'].apply(parse_vector)
    target_df['kws'] = target_df['keywords_jsonb'].apply(parse_keywords)
    print(f"ğŸ‘‰ ëŒ€ê¸°/ì‹ ê·œ ë¯¼ì› {len(target_df)}ê±´ ë¶„ë¥˜ ì‹œì‘...")

    # 3. ë§¤ì¹­ í”„ë¡œì„¸ìŠ¤
    assigned_count = 0
    unassigned_indices = []
    MATCH_THRESHOLD = 0.06 

    for idx, row in target_df.iterrows():
        best_match = None
        min_dist = 1.0
        for iid, info in incident_centroids.items():
            dist = calculate_hybrid_distance(row['vec'], info['vec'], row['kws'], info['kws'])
            if dist < min_dist:
                min_dist = dist
                best_match = iid
        
        if best_match and min_dist <= MATCH_THRESHOLD:
            cur.execute("UPDATE complaints SET incident_id = %s WHERE id = %s", (best_match, row['id']))
            cur.execute("UPDATE incidents SET complaint_count = complaint_count + 1, last_occurred = %s WHERE id = %s", (row['received_at'], best_match))
            assigned_count += 1
        else:
            unassigned_indices.append(idx)
    conn.commit()

    # 4. ì‹ ê·œ ì‚¬ê±´ ìƒì„±
    remaining_df = target_df.loc[unassigned_indices].copy()
    new_inc_count = 0
    
    if not remaining_df.empty and len(remaining_df) >= 2:
        vecs = np.stack(remaining_df['vec'].values)
        kws_list = remaining_df['kws'].tolist()
        n = len(kws_list)
        key_dist = np.ones((n, n))
        for i in range(n):
            for j in range(i, n):
                dist = 0.5 if not kws_list[i] and not kws_list[j] else \
                       1.0 if not kws_list[i] or not kws_list[j] else \
                       1.0 - (len(kws_list[i] & kws_list[j]) / len(kws_list[i] | kws_list[j]))
                key_dist[i, j] = key_dist[j, i] = dist

        sem_dist = cosine_distances(vecs)
        final_dist = (sem_dist * 0.8) + (key_dist * 0.2)
        
        dbscan = DBSCAN(eps=0.06, min_samples=2, metric='precomputed')
        labels = dbscan.fit_predict(final_dist)
        
        remaining_df['label'] = labels
        
        for label in set(labels):
            if label == -1: continue 
            cls = remaining_df[remaining_df['label'] == label]
            
            centroid_vec = np.mean(np.stack(cls['vec'].values), axis=0)
            
            # [ì¤‘ìš”] ì¤‘ë³µ ë°©ì§€ ì œëª© ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
            unique_title = generate_unique_smart_title(cls, centroid_vec, existing_titles)
            
            # ìƒì„±ëœ ì œëª©ì„ Setì— ì¦‰ì‹œ ë“±ë¡ (ì´ë²ˆ ë£¨í”„ ë‚´ì—ì„œ ë˜ ì•ˆ ê²¹ì¹˜ê²Œ)
            existing_titles.add(unique_title)
            
            rep_kw = get_representative_keyword(cls['kws'].tolist())
            kw_json = json.dumps([rep_kw], ensure_ascii=False)
            
            cur.execute("""
                INSERT INTO incidents (title, status, complaint_count, opened_at, closed_at, keywords)
                VALUES (%s, 'OPEN', %s, %s, %s, %s) RETURNING id
            """, (unique_title, len(cls), cls['received_at'].min(), cls['received_at'].max(), kw_json))
            
            new_iid = cur.fetchone()[0]
            ids = tuple(cls['id'].tolist())
            cur.execute(f"UPDATE complaints SET incident_id = %s WHERE id IN %s", (new_iid, ids))
            new_inc_count += 1
            
        conn.commit()

    cur.close(); conn.close()
    print(f"âœ… [ì™„ë£Œ] ê¸°ì¡´ë°© ì…ì¥: {assigned_count}ê±´ / ìƒˆ ë°© ê°œì„¤: {new_inc_count}ê°œ")

if __name__ == "__main__":
    run_cumulative_clustering()