import os
import glob
import pandas as pd
import random
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
from tqdm import tqdm

# [ì„¤ì •]
DATA_DIR = "data/processed_data"
DB_PATH = "complaint_vector_db"
TARGET_COUNT = 3000

def build_complaint_db():
    print("ğŸ—ï¸ êµ¬ì²­ë³„ ë¯¼ì› ë°ì´í„°ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤...")
    
    # ======================================================
    # âš¡ [í•µì‹¬ ìˆ˜ì •] ëš±ëš±í•œ llama3.1 ëŒ€ì‹  ë‚ ìŒ˜ëŒì´ nomic ëª¨ë¸ ì‚¬ìš©!
    # ======================================================
    print("ğŸš€ ê°€ë²¼ìš´ ì„ë² ë”© ëª¨ë¸(nomic-embed-text)ì„ ì¤€ë¹„í•©ë‹ˆë‹¤...")
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    
    # 1. íŒŒì¼ ì°¾ê¸°
    all_files = glob.glob(os.path.join(DATA_DIR, "*_cleaned.csv"))
    print(f"ğŸ“‚ ì´ {len(all_files)}ê°œì˜ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    all_documents = []

    # 2. ë°ì´í„° ì½ê¸°
    for file_path in all_files:
        try:
            df = pd.read_csv(file_path)
            file_name = os.path.basename(file_path)
            
            text_col = None
            for col in df.columns:
                if "ë‚´ìš©" in col or "content" in col:
                    text_col = col
                    break
            
            if text_col:
                for _, row in df.iterrows():
                    if pd.isna(row[text_col]) or len(str(row[text_col])) < 5:
                        continue
                        
                    meta = {"source": file_name}
                    if "category" in df.columns:
                        meta["category"] = row["category"]
                    
                    doc = Document(
                        page_content=str(row[text_col]),
                        metadata=meta
                    )
                    all_documents.append(doc)
        except Exception:
            pass 

    print(f"ğŸ“š ì›ë³¸ ë°ì´í„° ì´ {len(all_documents)}ê°œë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    
    if len(all_documents) > TARGET_COUNT:
        print(f"âœ‚ï¸ ëœë¤ìœ¼ë¡œ {TARGET_COUNT}ê°œë§Œ ë½‘ì•„ì„œ í•™ìŠµí•©ë‹ˆë‹¤.")
        random.shuffle(all_documents)
        all_documents = all_documents[:TARGET_COUNT]
    
    print("â³ ë²¡í„° ë³€í™˜ ì‹œì‘! (ì•„ê¹Œë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¼ ê²ë‹ˆë‹¤)")

    # 3. ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 500
    vectorstore = None
    
    for i in tqdm(range(0, len(all_documents), batch_size), desc="Vectorizing"):
        batch_docs = all_documents[i : i + batch_size]
        
        if vectorstore is None:
            if os.path.exists(DB_PATH):
                 vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
                 vectorstore.add_documents(batch_docs)
            else:
                vectorstore = Chroma.from_documents(batch_docs, embeddings, persist_directory=DB_PATH)
        else:
            vectorstore.add_documents(batch_docs)

    print(f"âœ… ë¯¼ì› DB êµ¬ì¶• ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {DB_PATH}")

if __name__ == "__main__":
    build_complaint_db()