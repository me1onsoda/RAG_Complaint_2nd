from langchain_ollama import ChatOllama

# 1. ë‚´ ì»´í“¨í„°ì— ìˆëŠ” Llama 3.1 ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
print("ğŸ¤– Llamaë¥¼ ê¹¨ìš°ëŠ” ì¤‘ì…ë‹ˆë‹¤... (ì»´í“¨í„° ì„±ëŠ¥ì— ë”°ë¼ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”)")
llm = ChatOllama(model="llama3.1", temperature=0)

# 2. ì§ˆë¬¸ì„ ë˜ì ¸ë´…ë‹ˆë‹¤.
question = "ì—¬ê¶Œ ë°œê¸‰ì€ ì–´ë””ì„œ í•´ì•¼ í•´? ì§§ê²Œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜."
print(f"â“ ì§ˆë¬¸: {question}")

# 3. ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤.
response = llm.invoke(question)

print("-" * 30)
print(f"ğŸ’¬ ë‹µë³€: {response.content}")
print("-" * 30)